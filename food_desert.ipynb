{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - The Battle of the Neighborhoods\n",
    "### Applied Data Science Capstone by IBM/Coursera\n",
    "##Table of Contents\n",
    "* [Introduction](#cd_introduction)\n",
    "* [Data](#cd_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"cd_introduction\"></a>\n",
    "In this project, we will try to find an optimal location for a grocery store. Specifically, we'll be looking in the capital region of New York State and the surrounding area. \n",
    "\n",
    "Since there are plenty of grocery stores in the capital region, we are seeking to identify areas that\n",
    "\n",
    "    a.) do not have a grocery store nearby, and\n",
    "    b.) have a sufficient population density to support a grocery store. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data <a name='cd_data'></a>\n",
    "Based on the above description, the factors that influence our decision are:\n",
    "    a.) The number of grocery stores in the area.\n",
    "    b.) The distance to each grocery store.\n",
    "\n",
    "Rather than using administrative boundaries (zipcode, county, city), we'll be generating the areas algorithmically. \n",
    "   \n",
    "Starting with Albany, NY and radiating outward we will generate equally spaced hexagons until we have covered Albany, Schenectady and Troy as well as some of the surrounding area. \n",
    "\n",
    "For each of those areas, we'll be calculating the number of businesses in general, and the number of grocery stores specifically. We'll be looking for areas where the number of grocery stores and the number of businesses diverge sharply. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T00:14:37.938877Z",
     "start_time": "2023-01-08T00:14:35.703319Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!conda install pyproj --y\n",
    "#!conda install shapely --y\n",
    "#!conda install geopy --y\n",
    "#!conda install folium --y\n",
    "#!conda install pyproj --y\n",
    "#!conda install shapely --y\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "import math\n",
    "import geopy\n",
    "import os\n",
    "import configparser\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import FloatImage\n",
    "import pyproj\n",
    "import shapely.geometry\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T00:14:37.954144Z",
     "start_time": "2023-01-08T00:14:37.938877Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a geocoder object with an arbitrary user agent\n",
    "\n",
    "geocoder = geopy.geocoders.Nominatim(user_agent='Capital_District_Notebook') \n",
    "\n",
    "# define a function to get geocoding from zip codes\n",
    "\n",
    "def get_lat_long_locale(zip_code, geocoder=geocoder):\n",
    "    \"\"\"\n",
    "    Takes a five character string representing the zip code, and returns \n",
    "    tuple representing the latitude and longitude of the zipcode center. \n",
    "    \"\"\"\n",
    "    location = geocoder.geocode({'postalcode':zip_code, \"country\":\"us\"})\n",
    "    \n",
    "    if location:\n",
    "        return (location.latitude, location.longitude, location.address)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Neighborhood Candidatates\n",
    "To get an initial sense of the geography, I'm using a list of zipcodes generated through a [tool provided by the US postal Service.]( \n",
    "https://www.unitedstateszipcodes.org/zip-code-radius-map.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T00:14:37.977228Z",
     "start_time": "2023-01-08T00:14:37.956130Z"
    }
   },
   "outputs": [],
   "source": [
    "zipcodes = [\n",
    "    12007, 12008, 12009, 12010, 12018, 12019, 12020, 12023, 12024, 12027,\n",
    "    12033, 12040, 12041, 12042, 12045, 12046, 12047, 12052, 12053, 12054,\n",
    "    12056, 12059, 12061, 12062, 12063, 12065, 12066, 12067, 12074, 12077,\n",
    "    12083, 12084, 12085, 12086, 12087, 12094, 12106, 12110, 12115, 12118,\n",
    "    12120, 12121, 12123, 12124, 12130, 12132, 12136, 12137, 12138, 12140,\n",
    "    12143, 12144, 12147, 12148, 12150, 12151, 12153, 12154, 12156, 12157,\n",
    "    12158, 12159, 12161, 12169, 12170, 12173, 12176, 12180, 12182, 12183,\n",
    "    12184, 12185, 12186, 12188, 12189, 12192, 12193, 12195, 12196, 12198,\n",
    "    12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211,\n",
    "    12222, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12863, \n",
    "    12866\n",
    "] \n",
    "def zip_format(zip:int) -> str :\n",
    "    \"\"\"\n",
    "    Takes zip codes as integers and returns 5 character strings.\n",
    "    \"\"\"\n",
    "    return f\"{zip:05d}\"\n",
    "\n",
    "zipcodes = list(map(zip_format, zipcodes)) # convert to strings\n",
    "\n",
    "zipcodes = pd.DataFrame(data=zipcodes, columns=['zipcodes'])\n",
    "zipcodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our zipcodes and our geocoder, we're going to use the geocoder to pull the center of each zipcode. \n",
    "\n",
    "Since this involves querying an API, we want to minimize the number of requests that we're making. The following block of code attempts to load the data from a pickle file first. If the pickle file doesn't exist, it will query the API and create the pickle file. Therefore, if we run this block repeatedly it will only execute the query once. Afterwards it will load the data from the pickle file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T00:14:38.009339Z",
     "start_time": "2023-01-08T00:14:37.977228Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file = 'zipcode_location.pickle'\n",
    "try:\n",
    "    # Try to read from a pickle cache\n",
    "    zipcodes = pd.read_pickle(file)\n",
    "    # TQDM is a library for outputing feedback on a running process.\n",
    "    # In this case, it will tell us that it's working from a locally\n",
    "    # cached file. \n",
    "    tqdm.write(f'Working from file, {file}')\n",
    "except:\n",
    "    # Here we use TQDM to show that we aren't able to load a cache,\n",
    "    # so.. we'll have to execute a query.\n",
    "    tqdm.write('File not found. Querying...') \n",
    "    \n",
    "    # tqdm modifies adds a method to the pandas DataFrame object called \n",
    "    # progress apply. Progress apply is like the apply function, \n",
    "    # but it includes a progress bar. \n",
    "    zipcodes['location'] = zipcodes['zipcodes'].progress_apply(\n",
    "        get_lat_long_locale)\n",
    "    \n",
    "    # Lastly, we write the file...\n",
    "    zipcodes.to_pickle(file)\n",
    "    \n",
    "    # ..and report out to the user.\n",
    "    tqdm.write(f'Writing file, {file}')\n",
    "\n",
    "zipcodes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is currently represented as zipcodes and location tuples. We want to convert our location tuples into latitude and longitude. To do that, we'll have to extract the latitude and longitude from the location column. \n",
    "\n",
    "We're going to do that with some simple throwaway functions (lambdas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T00:14:38.027360Z",
     "start_time": "2023-01-08T00:14:38.011334Z"
    }
   },
   "outputs": [],
   "source": [
    "zipcodes['latitude'] = zipcodes['location'].apply(lambda x: x[0])\n",
    "zipcodes['longitude'] = zipcodes['location'].apply(lambda x: x[1])\n",
    "\n",
    "\n",
    "# The first item is always the city or town\n",
    "zipcodes['city_or_town'] = zipcodes['location'].apply(\n",
    "    lambda x: x[2].split(',')[0].strip())\n",
    "\n",
    "# Extracting the remaining metadata is less straightforward. \n",
    "# Splitting the string and using negative indexing may look \n",
    "# a little strange, but it's the best way to get the correct values.\n",
    "\n",
    "# get the county\n",
    "zipcodes['county'] = zipcodes['location'].apply(\n",
    "     lambda x: x[2].split(',')[-4].strip())\n",
    "\n",
    "# Cleanup the value by chopping off the word county\n",
    "zipcodes['county'] = zipcodes['county'].apply(lambda x: x.split(' ')[:-1][0])\n",
    "\n",
    "# get the state\n",
    "zipcodes['state'] = zipcodes['location'].apply(\n",
    "     lambda x: x[2].split(',')[-3].strip())\n",
    "\n",
    "# cleanup the value by using the two character acronym\n",
    "zipcodes['state'].replace('New York', 'NY', inplace=True)\n",
    "\n",
    "# drop the original location column\n",
    "zipcodes.drop(columns=\"location\", inplace=True)\n",
    "zipcodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done that, we'll plot them on a map to get a better sense of **the area we intend to examine**.  This isn't the methodology that we outlined at the start. We're just trying to get a sense of the geography that we're working with. \n",
    "\n",
    "\n",
    "In the output that follows, red dots mark the zipcode center points. The green and purple circles mark is the approximate center of our area. The mean, and the median look to be virtually identical, meaning either variable should be useful for identifying our starting point. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T00:15:35.065345Z",
     "start_time": "2023-01-08T00:15:34.970823Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # delete the map if it already exists (useful when running/editing cells)\n",
    "    del capital_district_map\n",
    "except:\n",
    "    pass\n",
    "center_mean = (\n",
    "    zipcodes['longitude'].mean(), # approximate center of all our zipcodes\n",
    "    zipcodes['latitude'].mean(),\n",
    ")\n",
    "center_median = (\n",
    "    zipcodes['longitude'].median(), # approximate center of all our zipcodes\n",
    "    zipcodes['latitude'].median(),\n",
    ")\n",
    "\n",
    "capital_district_map = folium.Map(center_mean[::-1], zoom_start=9)  \n",
    "for ix, zipcode, lat, long, city, county, state in zipcodes.itertuples():\n",
    "    label = f'{zipcode}, {city}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker([lat, long],\n",
    "                        radius=2,\n",
    "                        popup=label,\n",
    "                        color='red',\n",
    "                        opacity=.5,\n",
    "                        fill=True,\n",
    "                        fill_color='orange',\n",
    "                        fill_opacity=0.1,\n",
    "                        parse_html=False).add_to(capital_district_map)\n",
    "\n",
    "folium.CircleMarker(center_mean[::-1],\n",
    "                    radius=20,\n",
    "                    popup='center',\n",
    "                    color='green',\n",
    "                    opacity=.5,\n",
    "                    fill=True,\n",
    "                    fill_color='green',\n",
    "                    fill_opacity=0.1,\n",
    "                    parse_html=False).add_to(capital_district_map)\n",
    "folium.CircleMarker(center_median[::-1],\n",
    "                    radius=20,\n",
    "                    popup='center',\n",
    "                    color='purple',\n",
    "                    opacity=.5,\n",
    "                    fill=True,\n",
    "                    fill_color='purple',\n",
    "                    fill_opacity=0.1,\n",
    "                    parse_html=False).add_to(capital_district_map)\n",
    "\n",
    "capital_district_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I stated before, the zip code data is just helpful for getting a frame of reference on our geography. We aren't going to rely on it for our final analysis. \n",
    "\n",
    "The reason for that decision is that the boundaries of zipcodes are fairly irregular. We want to be more fine grained. \n",
    "\n",
    "What we do next is create something like a hexagonal grid. We're using hexagons because they are compact. In a rectangular grid, the center point of each square would not be equidistant from each surrounding square. In a hexagonal grid, the centerpoint of each grid is equidistant from it's neighbors. \n",
    "\n",
    "To accurately calculate distances, however, we'll need to convert our data back and forth between long/lat coordinates and cartesian coordinates (in meters). The technique I'm using is demonstrated Data Scientist, Lyan Fu Fly. While his analysis focuses on the city of Berlin, this analysis focusses on an area of upstate NY. \n",
    "\n",
    "If you would like to read their analysis, you can find their medium article here: https://medium.com/@lyan.fu.fly/ibm-capstone-project-the-battle-of-neighborhoods-in-berlin-restaurants-dd326f1bfacb\n",
    "And their github repository here: https://github.com/lingyan-f/ly.fu-Coursera-IBM-Cap-final-battle-of-the-neighborhoods-of-berlin/blob/master/Final%20Capstone%20Project-ly.fu.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Projection Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T00:20:00.246498Z",
     "start_time": "2023-01-08T00:20:00.225493Z"
    }
   },
   "outputs": [],
   "source": [
    "def lonlat_to_xy(lon:float, lat:float) -> float:\n",
    "    \"\"\"\n",
    "    This function takes the longitude and latitude of a goegraphic point, \n",
    "    and converts it to x,y coordinates on a cartestian grid. In order to\n",
    "    convert the curved surface to a \"flat\" mapping, we need to select a \n",
    "    subsection of the globe and describe it as a rectangle. The pyproj library\n",
    "    helps us to accomplish this. \n",
    "    \"\"\"\n",
    "    # define the source and destination projects\n",
    "    src_proj = pyproj.Proj(proj='latlong', datum='WGS84')\n",
    "    dst_proj = pyproj.Proj(proj=\"utm\", zone=18, datum='WGS84')\n",
    "\n",
    "    # Create a transformer object to transform the coordinates\n",
    "    transformer = pyproj.transformer.Transformer.from_proj(src_proj, dst_proj)\n",
    "\n",
    "    # Transform the coordinates\n",
    "    x, y = transformer.transform(lon, lat)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def xy_to_lonlat(x:float, y:float) -> float:\n",
    "    \"\"\"\n",
    "    This function uses a cartesian representation of the globe to \n",
    "    convert back to latitude and longitude. It's a limited use function, \n",
    "    as it only works for zone 18... but for this project, we only need \n",
    "    zone 18. If we wanted to extend this project to encompass new \n",
    "    locales, or to repeat the analysis globally, we  would need to build a more\n",
    "    robust solution. \n",
    "    \"\"\"\n",
    "    # define the source and destination projects\n",
    "\n",
    "    src_proj= pyproj.Proj(proj=\"utm\", zone=18, datum='WGS84')\n",
    "    dst_proj = pyproj.Proj(proj='latlong', datum='WGS84')\n",
    "\n",
    "    # Create a transformer object to transform the coordinates\n",
    "    transformer = pyproj.transformer.Transformer.from_proj(src_proj, dst_proj)\n",
    "\n",
    "    # Transform the coordinates\n",
    "    lon, lat = transformer.transform(x, y)\n",
    "    \n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "\n",
    "def calc_xy_distance(x1:float, y1:float, x2:float, y2:float) -> float:\n",
    "    \"\"\"\n",
    "    Given two points, return the euclidean distance.\n",
    "    \"\"\"\n",
    "    diff_x = x2 - x1\n",
    "    diff_y = y2 - y1\n",
    "    return np.sqrt(diff_x ** 2 + diff_y **2)\n",
    "\n",
    "\"\"\"\n",
    "Testing that our functions are working as intended. If we get can convert from\n",
    "latitude and longitude and back again (getting our original values) then we are \n",
    "good shape. \n",
    "\"\"\"\n",
    "print('Coordinate transformation check')\n",
    "print('-------------------------------')\n",
    "print('Capital District longitude={}, latitude={}'.format(*center_mean))\n",
    "x, y = lonlat_to_xy(*center_mean)\n",
    "print('Capital District UTM X={}, Y={}'.format(x, y))\n",
    "lo, la = xy_to_lonlat(x, y)\n",
    "print('Capital District  longitude={}, latitude={}'.format(lo, la))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our coordinate functions are working. We can convert to and from longitude and latitude and we get back the orignal values. \n",
    "\n",
    "Our next step is to create a grid of points that are related hexagonally.  \n",
    "\n",
    "To create a **hexagonal grid of cells**: we offset every other row, and adjust vertical row spacing so that **every cell center is equally distant from all it's neighbors**. Another way of discribing this is to say that each point has six neighboring points and that each neighboring point is located on a thirty degree line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T01:00:56.256417Z",
     "start_time": "2023-01-08T01:00:54.832896Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del latitudes\n",
    "    del longitudes\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_hex_grid(center=center_mean):\n",
    "    # first define our city center in Cartesian coordinates\n",
    "    center_x, center_y = lonlat_to_xy(*center)  \n",
    "\n",
    "    \n",
    "    # k is the coef of a 30 degree angle on the unit circle\n",
    "    \n",
    "    k = math.sqrt(3) / 2 \n",
    "    \n",
    "    # get our minimum x value. In this case, we're starting  \n",
    "    # 50 kilometers west of the center point\n",
    "    x_min = center_x - 50000 \n",
    "    \n",
    "    x_step = 2500 # two and a half kilometers increments\n",
    "    \n",
    "    # because the hexagons \"settle into\" the crevices of the row beneath them, \n",
    "    # we can fit more hexagons on the vertical axis than the horizontal axis\n",
    "    \n",
    "    y_min = center_y - 50000 - (int(126 / k) * k * 2500 - 100000) / 2\n",
    "    y_step = 2500 * k # 2 and a half kilometers * sqrt(3)/2\n",
    "\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    distances_from_center = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "    # iterate over y rows as [i]\n",
    "    for i in range(0, int(126 / k)):\n",
    "        \n",
    "        # First we identify our vertical incrementation to locate \n",
    "        # our row.\n",
    "        # In this case y is equal to the minimum plus an incrementation\n",
    "        # based on the y_step value. \n",
    "        y = y_min + i * y_step\n",
    "        \n",
    "        # the row is shifted right by a half hexagon when the row is even\n",
    "        x_offset = 1250 if i % 2 == 0 else 0\n",
    "        \n",
    "        for j in range(0, 126):\n",
    "            # Now that we have identified the center of our row\n",
    "            # we derive a simpler calculation for the east/west\n",
    "            # positioning\n",
    "            x = x_min + j * x_step + x_offset\n",
    "            distance_from_center = calc_xy_distance(center_x, center_y, x, y)\n",
    "            # this step  is a bit of a brute force method. \n",
    "            # we're cutting off any points that are more than 50.05 kilometers\n",
    "            # from the center of our points. This effectively turns our grid into \n",
    "            # a circle. \n",
    "            if (distance_from_center <= 50050):\n",
    "                lon, lat = xy_to_lonlat(x, y)\n",
    "                latitudes.append(lat)\n",
    "                longitudes.append(lon)\n",
    "                distances_from_center.append(distance_from_center)\n",
    "                xs.append(x)\n",
    "                ys.append(y)\n",
    "    # lastly we construct a dataframe\n",
    "    neighborhoods = pd.DataFrame({\n",
    "        'latitude': latitudes,\n",
    "        'longitude': longitudes,\n",
    "        'x': xs,\n",
    "        'y': ys,\n",
    "        'distance': distances_from_center\n",
    "    })\n",
    "    # and return it\n",
    "    return neighborhoods\n",
    "\n",
    "\n",
    "neighborhoods = generate_hex_grid(center_mean)\n",
    "neighborhoods.to_csv('neighborhoods.csv')\n",
    "\n",
    "neighborhoods.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step shows the results of our work. We use the hexagonal positioning to place densely packed circles onto our map. \n",
    "\n",
    "The geography includes rural, suburban and urban regions. It should be helpful for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T01:02:54.858696Z",
     "start_time": "2023-01-08T01:02:53.970431Z"
    }
   },
   "outputs": [],
   "source": [
    "hex_map = folium.Map(location=center_mean[::-1], zoom_start=9)\n",
    "\n",
    "folium.Circle(center_mean[::-1], radius=50050).add_to(hex_map)\n",
    "\n",
    "# Iterate across the center of each \"hex\" and draw a circle around it. The radius of the circle is 1.25k. \n",
    "# If you recall from the prior functions, our neighborhoods are spaced with a diameter of 2.5k. \n",
    "for _, lat, long in neighborhoods[['latitude', 'longitude']].itertuples():\n",
    "    label = f'{lat, long}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Circle([lat, long],\n",
    "                  radius=1250,\n",
    "                  popup=label,\n",
    "                  color='red',\n",
    "                  opacity=.1,\n",
    "                  fill=True,\n",
    "                  fill_color='red',\n",
    "                  fill_opacity=0.01,\n",
    "                  parse_html=False).add_to(hex_map)\n",
    "\n",
    "hex_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick glimpse at what our coordinate system looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T01:02:59.552189Z",
     "start_time": "2023-01-08T01:02:59.530055Z"
    }
   },
   "outputs": [],
   "source": [
    "neighborhoods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The real data\n",
    "Finally. We're getting somewhere. In the next few steps we're getting ready \n",
    "to query the foursquare api. \n",
    "\n",
    "Unfortunately, this part requires some security precautions. I have my credentials stored in a .env \n",
    "file which probably won't be available to you. If you would like to run this on your own, you can \n",
    "setup a developer account and create your own version of the file. \n",
    "\n",
    "The file should be named \".env\" and stored in the same directory as this notebook. After you \n",
    "create the file paste in the following:\n",
    "```\n",
    "[foursquare]\n",
    "client_id=yourKeyHere\n",
    "client_secret=yourSecretHere\n",
    " ```\n",
    "Then, replace the relevant parts with your own credentials. If you need to register for a developer account, you can do that [here](https://foursquare.com/developers/signup).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T01:03:03.449634Z",
     "start_time": "2023-01-08T01:03:03.415765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the foursquare API credentials from a \n",
    "# .env file.\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('.env'))\n",
    "\n",
    "# The url of the foursquare api\n",
    "url = 'https://api.foursquare.com/v2/venues/explore'\n",
    "\n",
    "# Loading our secrets from a .env file\n",
    "\n",
    "fsq_params = {\n",
    "    'client_id': config.get('foursquare', 'client_id'),\n",
    "    'client_secret': config.get('foursquare', 'client_secret'),\n",
    "    #'v': '20200222'\n",
    "}\n",
    "\n",
    "\n",
    "def query_fsquare_for_nearby_venues(lat,\n",
    "                                    long,\n",
    "                                    limit=2000,\n",
    "                                    radius=20000,\n",
    "                                    credentials=fsq_params,\n",
    "                                    check_cache=False):\n",
    "    \"\"\"\n",
    "    Query the foursquare API using latitude and longitude, return venues that \n",
    "    are withing a given radius of the point. This function first checks for a \n",
    "    locally available csv (which indicates that the function has already been run).\n",
    "    If no such csv is available, it executes the query. \n",
    "    \"\"\"\n",
    "    if check_cache:\n",
    "        cache = pd.read_csv(check_cache,\n",
    "                            usecols=['center_lat',\n",
    "                                     'center_long']).drop_duplicates()\n",
    "        in_cache = cache[(cache['center_lat'] == lat)\n",
    "                         & (cache['center_long'] == long)]\n",
    "\n",
    "        if in_cache.empty:\n",
    "            credentials['ll'] = '{},{}'.format(lat, long),\n",
    "            credentials['limit'] = limit,\n",
    "            credentials['radius'] = radius\n",
    "            resp = requests.get(url=url, params=credentials)\n",
    "            return {'lat': lat, 'long': long, 'resp': resp}\n",
    "    else:\n",
    "        credentials['ll'] = '{},{}'.format(lat, long),\n",
    "        credentials['limit'] = limit,\n",
    "        credentials['radius'] = radius\n",
    "        resp = requests.get(url=url, params=credentials)\n",
    "        return {'lat': lat, 'long': long, 'resp': resp}\n",
    "\n",
    "\n",
    "def normalize_response(response):\n",
    "    \"\"\"\n",
    "    A helper function to clean up the response from the \n",
    "    query_fsquare_for_nearby_venues function.\n",
    "    \n",
    "    This extracts the results from json and processes them into a dataframe. \n",
    "    \"\"\"\n",
    "    results = response['resp'].json()\n",
    "    results = pd.json_normalize(results['response']['groups'][0]['items'])\n",
    "    try:\n",
    "        results = results[[\n",
    "            'venue.name', 'venue.location.lat', 'venue.location.lng',\n",
    "            \"venue.categories\"\n",
    "        ]]\n",
    "        results.iloc[:, -1] = results['venue.categories'].map(\n",
    "            lambda x: x[0]['shortName'])\n",
    "    except KeyError as e:\n",
    "        #mock response venue\n",
    "        results = pd.DataFrame(\n",
    "            {\n",
    "                'venue.name': 'None',\n",
    "                'venue.location.lat': response['lat'],\n",
    "                'venue.location.lng': response['long'],\n",
    "                'venue.categories': 'None'\n",
    "            },\n",
    "            index=[0])\n",
    "    results['center_lat'] = response['lat']\n",
    "    results['center_long'] = response['long']\n",
    "    results.columns = [\n",
    "        'venue', 'latitude', 'longitude', 'category', 'center_lat',\n",
    "        'center_long'\n",
    "    ]\n",
    "    return results\n",
    "\n",
    "\n",
    "def cache_successful_response(successful_response_data):\n",
    "    \"\"\"\n",
    "    Is this necessary?\n",
    "    \"\"\"\n",
    "    try:  #look for existing cache\n",
    "        previously_cached_data = pd.read_csv('response200.csv', index_col=0)\n",
    "        new_records = len(successful_response_data)\n",
    "        old_records = len(previously_cached_data)\n",
    "        results = pd.concat([successful_response_data, previously_cached_data])\n",
    "        results.drop_duplicates(inplace=True)\n",
    "        non_duplicates = len(results) - old_records\n",
    "        results.to_csv('response200.csv')\n",
    "        return old_records, new_records, non_duplicates\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        tqdm.write(str(e))\n",
    "        successful_response_data.to_csv('response200.csv')\n",
    "        return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:35:46.778760Z",
     "start_time": "2020-03-15T16:22:43.524420Z"
    }
   },
   "outputs": [],
   "source": [
    "fails = []\n",
    "check_cache = False\n",
    "\n",
    "for ix, lat, long in tqdm(\n",
    "        list(neighborhoods[['latitude', 'longitude']].itertuples())):\n",
    "    q = query_fsquare_for_nearby_venues(lat, long, check_cache=check_cache)\n",
    "    \n",
    "    if q is None:\n",
    "        # If we don't get a valid response from foursquer\n",
    "        next\n",
    "    elif q['resp'].status_code == 200:\n",
    "        #if we do get a valid response, normalize it.Then check for duplicates. Then \n",
    "        success = normalize_response(q)\n",
    "        old, new, nondupe = cache_successful_response(success)\n",
    "        #tqdm.write('{} records in cache. Comparing {}. {} new records'.format(old, new, nondupe))\n",
    "    elif q['resp']:\n",
    "        tqdm.write('{} {} {}'.format(q['lat'], q['long'],\n",
    "                                     q['resp'].status_code))\n",
    "        fails.append((lat, long, q['resp'].status_code))\n",
    "    check_cache = \"response200.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:45:20.125616Z",
     "start_time": "2020-03-15T16:45:20.103443Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking file size and whether any query point failed\n",
    "fsq = pd.read_csv('response200.csv', index_col=0)\n",
    "print(f\"\"\"\n",
    "shape of dataset: {fsq.shape}\n",
    "shape of points dataset {neighborhoods.shape}\n",
    "number of unique centers in new dataset:{fsq[\"center_lat\"].nunique(), fsq[\"center_long\"].nunique()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:45:50.394928Z",
     "start_time": "2020-03-15T16:45:50.201615Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking head\n",
    "fsq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq.groupby([\"center_lat\", \"center_long\"]).count().sort_values(\"venue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:46:27.132356Z",
     "start_time": "2020-03-15T16:46:27.114740Z"
    }
   },
   "outputs": [],
   "source": [
    "fsq[fsq.category.str.contains(\"Grocery\")].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq.category.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many categories to get our clustering algorithm going, and doing text analysis is a little beyond the scope of this project.\n",
    "\n",
    "I've opted to create a mapping file that categorizes venues into a few groups:\n",
    "\n",
    "* Convenience (Corner stores, bodegas, delis, etc.)\n",
    "* Food (Restauarants, or convenience food)\n",
    "* Grocery (Grocery stores & supermarkets)\n",
    "* Outdoors (Parks and natural features, or poi that indicate access to a natural feature)\n",
    "* Entertainment (Movies, Theatres, Museums, Stadiums and Nightlife(other than food)\n",
    "* Retail (Most stores)\n",
    "* Commercial (Services, or business to business stores)\n",
    "* Other (Things that didn't fit well)\n",
    "\n",
    "\n",
    "There's a bit of rough justice in selecting these categories. We're going to see how they work out, and if they work poorly we'll revisit them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open our mapping file and create a dictionary to reduce the categories\n",
    "with open(\"mapping\") as f:\n",
    "    \n",
    "    mapping ={item[0]:item[1] for item in [ln.strip().split(':') for ln in f]}\n",
    "    \n",
    "    # Café isn't working right, probably beccause of the accented character. \n",
    "    # Something to look at more closely when I have the time. \n",
    "    # For now, just fix it. \n",
    "    mapping[\"Café\"] = \"Food\"\n",
    "\n",
    "\n",
    "fsq[\"sec_category\"] = fsq[\"category\"]\n",
    "fsq[\"category\"] = fsq.category.replace(mapping)\n",
    "\n",
    "\n",
    "fsq = fsq[fsq[\"category\"].isin((fsq.category.value_counts() > 3).index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = fsq.category.value_counts()\n",
    "category_counts = category_counts[category_counts>3]\n",
    "fsq = fsq[fsq[\"category\"].isin(category_counts.index)]\n",
    "fsq.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to plot our venues out to our map. If we do it well, we should start to see urban areas show up darker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:46:35.353457Z",
     "start_time": "2020-03-15T16:46:28.619991Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del capital_district_locations\n",
    "except:\n",
    "    pass\n",
    "capital_district_locations = folium.Map(center_mean[::-1], zoom_start=9)\n",
    "for _, venue, lat, long, category, clat, clong, sec_category in tqdm(list(fsq.itertuples())):\n",
    "    label = f'{category}, {sec_category}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    \n",
    "    folium.CircleMarker([lat, long],\n",
    "                        radius=2,\n",
    "                        popup=label,\n",
    "                        color='blue',\n",
    "                        opacity=.15,\n",
    "                        fill=True,\n",
    "                        fill_color='lightblue',\n",
    "                        fill_opacity=0.1,\n",
    "                        parse_html=False).add_to(capital_district_locations)\n",
    "\n",
    "capital_district_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was pretty succcessful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "\"Food\":\"red\",\n",
    "\"Retail\":\"orange\",\n",
    "\"Commercial\":\"blue\",\n",
    "\"Outdoors\":\"green\",\n",
    "\"Entertainment\":\"yellow\",\n",
    "\"Convenience\":\"white\",\n",
    "\"Grocery\":\"cyan\",\n",
    "\"Other\":\"purple\",\n",
    "\"Rural\":\"brown\",\n",
    "\"Gas/Highway/Reststop\":\"black\"}\n",
    "\n",
    "try:\n",
    "    del capital_district_locations\n",
    "except:\n",
    "    pass\n",
    "capital_district_locations = folium.Map(center_mean[::-1], zoom_start=9)\n",
    "for _, venue, lat, long, category, clat, clong, sec_category in tqdm(list(fsq.itertuples())):\n",
    "    label = f'{category}, {sec_category}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    \n",
    "    folium.CircleMarker([lat, long],\n",
    "                        radius=2,\n",
    "                        popup=label,\n",
    "                        color=colors[category],\n",
    "                        opacity=.15,\n",
    "                        fill=True,\n",
    "                        fill_color=colors[category],\n",
    "                        fill_opacity=0.1,\n",
    "                        parse_html=False).add_to(capital_district_locations)\n",
    "\n",
    "capital_district_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "\"Food\":\"red\",\n",
    "\"Retail\":\"blue\",\n",
    "\"Commercial\":\"blue\",\n",
    "\"Outdoors\":\"green\",\n",
    "\"Entertainment\":\"blue\",\n",
    "\"Convenience\":\"white\",\n",
    "\"Grocery\":\"cyan\",\n",
    "\"Other\":\"purple\",\n",
    "\"Rural\":\"purple\",\n",
    "\"Gas/Highway/Reststop\":\"black\"}\n",
    "\n",
    "try:\n",
    "    del capital_district_locations\n",
    "except:\n",
    "    pass\n",
    "capital_district_locations = folium.Map(center_mean[::-1], zoom_start=9)\n",
    "for _, venue, lat, long, category, clat, clong, sec_category in tqdm(list(fsq.itertuples())):\n",
    "    label = f'{category}, {sec_category}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    \n",
    "    folium.CircleMarker([lat, long],\n",
    "                        radius=2,\n",
    "                        popup=label,\n",
    "                        color=colors[category],\n",
    "                        opacity=.15,\n",
    "                        fill=True,\n",
    "                        fill_color=colors[category],\n",
    "                        fill_opacity=0.1,\n",
    "                        parse_html=False).add_to(capital_district_locations)\n",
    "\n",
    "capital_district_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:46:46.981155Z",
     "start_time": "2020-03-15T16:46:40.363489Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del grocery_map\n",
    "except:\n",
    "    pass\n",
    "grocery_map = folium.Map(center_mean[::-1], zoom_start=9)\n",
    "\n",
    "for _, venue, lat, long, category, clat, clong, sec_category in tqdm(list(fsq.itertuples())):\n",
    "    label = f'{category}, {sec_category}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    \n",
    "    if category != 'Grocery':\n",
    "        folium.CircleMarker([lat, long],\n",
    "                            radius=3,\n",
    "                            popup=label,\n",
    "                            color='purple',\n",
    "                            opacity=.025,\n",
    "                            fill=True,\n",
    "                            fill_color='purple',\n",
    "                            fill_opacity=0.1,\n",
    "                            parse_html=False).add_to(grocery_map)\n",
    "    else:\n",
    "        \n",
    "        folium.CircleMarker([lat, long],\n",
    "                            radius=10,\n",
    "                            popup=label,\n",
    "                            color='green',\n",
    "                            opacity=.5,\n",
    "                            fill=True,\n",
    "                            fill_color='green',\n",
    "                            fill_opacity=0.1,\n",
    "                            parse_html=False).add_to(grocery_map)\n",
    "\n",
    "grocery_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T18:58:28.277938Z",
     "start_time": "2020-03-14T18:58:28.275556Z"
    }
   },
   "source": [
    "# First attempt at modeling\n",
    "\n",
    "To generate our features we're going to create dummies from our primary categories and then merge them back onto the original dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:47:26.176277Z",
     "start_time": "2020-03-15T16:47:26.131066Z"
    }
   },
   "outputs": [],
   "source": [
    "fsq = pd.concat([fsq, pd.get_dummies(fsq.category)], axis=1)\n",
    "\n",
    "fsq = fsq.loc[:,~fsq.columns.duplicated()] # If the cell is accidentally run more than once, this will drop the duplicate columns. \n",
    "fsq[\"poi\"] = pd.get_dummies(fsq.category).sum(axis=1)\n",
    "fsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg=     fsq.groupby(['center_lat', 'center_long'])[\"Commercial\", \"Convenience\", \"Entertainment\", \n",
    "                                                    \"Food\", \"Gas/Highway/Reststop\", \"Grocery\", \n",
    "                                                    \"Other\", \"Outdoors\", \"Retail\", \n",
    "                                                    \"Rural\", \"poi\"].agg([\"mean\", \"sum\"])\n",
    "agg.columns = agg.columns.to_flat_index()\n",
    "agg.columns = [f\"{col[0]}_{col[1]}\" for col in agg.columns]\n",
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize, and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:47:29.213131Z",
     "start_time": "2020-03-15T16:47:28.915210Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled = agg.copy()\n",
    "\n",
    "scaled.iloc[:,:] = StandardScaler().fit_transform(X=agg.iloc[:, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:58:10.997446Z",
     "start_time": "2020-03-15T16:58:09.706532Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ks = list(range(1, 15))\n",
    "sse = []\n",
    "for k in ks:\n",
    "    kmm = KMeans(n_clusters=k)\n",
    "    kmm.fit(scaled.iloc[:, :21])\n",
    "\n",
    "    sse.append(kmm.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(ks, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset doesn't seem to like the standard approach to kmeans. \n",
    "\n",
    "There's no clear elbow in our scree plot. We'll try and do a little visual inspection by collapsing our data into two dimensions, and evaluating the clusters visually. If we were to make a guess from this, we have at most 4 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_outp = pd.DataFrame(PCA(2).fit_transform(scaled.iloc[:, :21]))\n",
    "pca_outp.columns = \"X\", \"Y\"\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10,10) )\n",
    "M = KMeans\n",
    "for k, ax in zip(range(2, 8), axs.flatten(), ):\n",
    "    m=M(k)\n",
    "    m.fit(scaled.iloc[:,:21])\n",
    "    ypred= m.fit_predict(scaled.iloc[:, :21])\n",
    "    sc = silhouette_score(scaled.iloc[:,:21],ypred, )\n",
    "    sns.scatterplot(x=\"X\", y=\"Y\", data=pca_outp, c=ypred, ax=ax)\n",
    "    ax.set_title(f\"k={k}, silhoute:{sc}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled.iloc[features]\n",
    "pca_outp = pd.DataFrame(PCA(2).fit_transform(X))\n",
    "pca_outp.columns = \"X\", \"Y\"\n",
    "trials =16\n",
    "fig, axs = plt.subplots(nrows=trials//2, ncols=2, figsize=(10,trials*3) )\n",
    "for eps, ax in zip(np.linspace(2.2,2.5,trials), axs.flatten(), ):\n",
    "    m=DBSCAN(eps=eps, )\n",
    "    ypred= m.fit_predict(scaled.iloc[:, :21])\n",
    "    sc = silhouette_score(scaled.iloc[:,:21],ypred, )\n",
    "    sns.scatterplot(x=\"X\", y=\"Y\", data=pca_outp, c=ypred, ax=ax)\n",
    "    ax.set_title(f\"eps={eps:.4f}, silhoute:{sc:.4f}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled.iloc[features]\n",
    "pca_outp = pd.DataFrame(PCA(2).fit_transform(X))\n",
    "pca_outp.columns = \"X\", \"Y\"\n",
    "from itertools import product\n",
    "\n",
    "trials = product([\n",
    "fig, axs = plt.subplots(nrows=trials//2, ncols=2, figsize=(10,trials*3) )\n",
    "\n",
    "for eps, ax in zip(np.linspace(2.2,2.5,trials), axs.flatten(), ):\n",
    "    m=Birch()\n",
    "    ypred= m.fit_predict(scaled.iloc[:, :21])\n",
    "    sc = silhouette_score(scaled.iloc[:,:21],ypred, )\n",
    "    sns.scatterplot(x=\"X\", y=\"Y\", data=pca_outp, c=ypred, ax=ax)\n",
    "    ax.set_title(f\"eps={eps:.4f}, silhoute:{sc:.4f}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = scaled.iloc[:,:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing distinguishing features between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:59:11.758353Z",
     "start_time": "2020-03-15T16:59:11.751405Z"
    }
   },
   "outputs": [],
   "source": [
    "x_scale = agg.max().max()  # for setting a common x-axis scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:59:13.726612Z",
     "start_time": "2020-03-15T16:59:12.209306Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = list(range(agg.KMeans_5.nunique()))\n",
    "\n",
    "cluster_box_plots = {n: n for n in clusters}\n",
    "for cluster in clusters:\n",
    "    cluster_summary = agg[agg['KMeans_5'] == cluster]\n",
    "    cluster_summary = cluster_summary.drop(columns=['KMeans_5'])\n",
    "    top5_filter = cluster_summary.mean().sort_values(ascending=False)[:5].index\n",
    "    cluster_top5 = cluster_summary[top5_filter]\n",
    "\n",
    "    title = \"Cluster_{}\".format(cluster)\n",
    "    plt.figure(figsize=(4, 2.5))\n",
    "    plt.tight_layout()\n",
    "    cluster_box_plots[cluster] = sns.boxplot(data=cluster_top5,\n",
    "                                             orient='h',\n",
    "                                             palette=\"Set2\")\n",
    "    cluster_box_plots[cluster].set_title(title)\n",
    "    cluster_box_plots[cluster].set_xbound(0, x_scale)\n",
    "    cluster_box_plots[cluster].set_xticks([])\n",
    "    cluster_box_plots[cluster].figure.savefig('{}.png'.format(title),\n",
    "                                              bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Clusters Geographically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:06:35.472858Z",
     "start_time": "2020-03-15T17:06:33.751809Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del results_map\n",
    "except:\n",
    "    pass\n",
    "colors = {\n",
    "    0: 'red',\n",
    "    1: 'green',\n",
    "    2: 'navy',\n",
    "    3: 'orange',\n",
    "    4: 'violet',\n",
    "    5: \"yellow\",\n",
    "}\n",
    "\n",
    "results_map = folium.Map(location=center[::-1], zoom_start=9)\n",
    "\n",
    "folium.Circle(center[::-1], radius=50050).add_to(results_map)\n",
    "\n",
    "for latlong, cluster in scored_profile['cluster'].iteritems():\n",
    "    label = f'{latlong}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Circle(latlong,\n",
    "                  radius=1200 * 1.2,\n",
    "                  popup=label,\n",
    "                  color=colors[cluster],\n",
    "                  opacity=.1,\n",
    "                  fill=True,\n",
    "                  fill_color=colors[cluster],\n",
    "                  fill_opacity=0.25,\n",
    "                  parse_html=False).add_to(results_map)\n",
    "results_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T16:59:17.434956Z",
     "start_time": "2020-03-15T16:59:17.431320Z"
    }
   },
   "outputs": [],
   "source": [
    "offset_center = center[0] + .2, center[1] + .05,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:09:15.838463Z",
     "start_time": "2020-03-15T17:09:15.472099Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del cluster0_map\n",
    "except:\n",
    "    pass\n",
    "\n",
    "cluster0_map = folium.Map(location=offset_center[::-1], zoom_start=9)\n",
    "cluster0_iterable = scored_profile[scored_profile['cluster'] == 0]\n",
    "folium.Circle(center[::-1], radius=50050).add_to(cluster0_map)\n",
    "\n",
    "for latlong in cluster0_iterable.index:\n",
    "    label = f'{latlong}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Circle(latlong,\n",
    "                  radius=1200 * 1.2,\n",
    "                  popup=label,\n",
    "                  color=colors[cluster],\n",
    "                  opacity=.1,\n",
    "                  fill=True,\n",
    "                  fill_color=colors[cluster],\n",
    "                  fill_opacity=0.25,\n",
    "                  parse_html=False).add_to(cluster0_map)\n",
    "\n",
    "FloatImage(\"Cluster_0.png\", bottom=70, left=62).add_to(cluster0_map)\n",
    "cluster0_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:09:17.688300Z",
     "start_time": "2020-03-15T17:09:17.614757Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del cluster1_map\n",
    "except:\n",
    "    pass\n",
    "\n",
    "cluster1_map = folium.Map(location=offset_center[::-1], zoom_start=9)\n",
    "cluster1_iterable = scored_profile[scored_profile['cluster'] == 1]\n",
    "\n",
    "folium.Circle(center[::-1], radius=50050).add_to(cluster1_map)\n",
    "\n",
    "for latlong in cluster1_iterable.index:\n",
    "    label = f'{latlong}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Circle(latlong,\n",
    "                  radius=1200 * 1.2,\n",
    "                  popup=label,\n",
    "                  color=colors[1],\n",
    "                  opacity=.1,\n",
    "                  fill=True,\n",
    "                  fill_color=colors[1],\n",
    "                  fill_opacity=0.25,\n",
    "                  parse_html=False).add_to(cluster1_map)\n",
    "\n",
    "FloatImage(\"Cluster_1.png\", bottom=70, left=62).add_to(cluster1_map)\n",
    "cluster1_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:09:19.547078Z",
     "start_time": "2020-03-15T17:09:19.295342Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del cluster2_map\n",
    "except:\n",
    "    pass\n",
    "\n",
    "cluster2_map = folium.Map(location=offset_center[::-1], zoom_start=9)\n",
    "cluster2_iterable = scored_profile[scored_profile['cluster'] == 2]\n",
    "\n",
    "folium.Circle(center[::-1], radius=50050).add_to(cluster2_map)\n",
    "\n",
    "for latlong in cluster2_iterable.index:\n",
    "    label = f'{latlong}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Circle(latlong,\n",
    "                  radius=1200 * 1.2,\n",
    "                  popup=label,\n",
    "                  color=colors[2],\n",
    "                  opacity=.1,\n",
    "                  fill=True,\n",
    "                  fill_color=colors[2],\n",
    "                  fill_opacity=0.25,\n",
    "                  parse_html=False).add_to(cluster2_map)\n",
    "\n",
    "FloatImage(\"Cluster_2.png\", bottom=70, left=62).add_to(cluster2_map)\n",
    "cluster2_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:09:26.644447Z",
     "start_time": "2020-03-15T17:09:26.437212Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del cluster3_map\n",
    "except:\n",
    "    pass\n",
    "\n",
    "cluster3_map = folium.Map(location=offset_center[::-1], zoom_start=9)\n",
    "cluster3_iterable = scored_profile[scored_profile['cluster'] == 3]\n",
    "\n",
    "folium.Circle(center[::-1], radius=50050).add_to(cluster3_map)\n",
    "\n",
    "for latlong in cluster3_iterable.index:\n",
    "    label = f'{latlong}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Circle(latlong,\n",
    "                  radius=1200 * 1.2,\n",
    "                  popup=label,\n",
    "                  color=colors[3],\n",
    "                  opacity=.1,\n",
    "                  fill=True,\n",
    "                  fill_color=colors[3],\n",
    "                  fill_opacity=0.25,\n",
    "                  parse_html=False).add_to(cluster3_map)\n",
    "\n",
    "FloatImage(\"Cluster_3.png\", bottom=70, left=62).add_to(cluster3_map)\n",
    "cluster3_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:09:28.739254Z",
     "start_time": "2020-03-15T17:09:27.893924Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del cluster4_map\n",
    "except:\n",
    "    pass\n",
    "\n",
    "cluster4_map = folium.Map(location=offset_center[::-1], zoom_start=9)\n",
    "cluster4_iterable = scored_profile[scored_profile['cluster'] == 4]\n",
    "\n",
    "folium.Circle(center[::-1], radius=50050).add_to(cluster4_map)\n",
    "\n",
    "for latlong in cluster4_iterable.index:\n",
    "    label = f'{latlong}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Circle(latlong,\n",
    "                  radius=1200 * 1.2,\n",
    "                  popup=label,\n",
    "                  color=colors[4],\n",
    "                  opacity=.1,\n",
    "                  fill=True,\n",
    "                  fill_color=colors[4],\n",
    "                  fill_opacity=0.25,\n",
    "                  parse_html=False).add_to(cluster4_map)\n",
    "\n",
    "FloatImage(\"Cluster_4.png\", bottom=70, left=62).add_to(cluster4_map)\n",
    "cluster4_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:09:48.807207Z",
     "start_time": "2020-03-15T17:09:48.804205Z"
    }
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     del cluster5_map\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# cluster5_map = folium.Map(location=offset_center[::-1], zoom_start=9)\n",
    "# cluster5_iterable = scored_profile[scored_profile['cluster']==5]\n",
    "\n",
    "# folium.Circle(center[::-1], radius=50050).add_to(cluster5_map)\n",
    "\n",
    "# for latlong in cluster5_iterable.index:\n",
    "#     label = f'{latlong}'\n",
    "#     label = folium.Popup(label, parse_html=True)\n",
    "#     folium.Circle(latlong,\n",
    "#                   radius=1200 * 1.2,\n",
    "#                   popup=label,\n",
    "#                   color=colors[5],\n",
    "#                   opacity=.1,\n",
    "#                   fill=True,\n",
    "#                   fill_color=colors[5],\n",
    "#                   fill_opacity=0.25,\n",
    "#                   parse_html=False).add_to(cluster5_map)\n",
    "\n",
    "# FloatImage(\"Cluster_5.png\", bottom=70, left=62 ).add_to(cluster5_map)\n",
    "# cluster5_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:10:52.572054Z",
     "start_time": "2020-03-15T17:10:52.556413Z"
    }
   },
   "outputs": [],
   "source": [
    "venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:09:58.676797Z",
     "start_time": "2020-03-15T17:09:56.830642Z"
    }
   },
   "outputs": [],
   "source": [
    "for a, b, lat, long, category, generic, in tqdm(list(venues.itertuples())):\n",
    "    label = f'{generic}, {category}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    if generic == 'Groceries':\n",
    "        folium.Circle([lat, long], radius=625, color='Black',\n",
    "                      fill=False).add_to(results_map)\n",
    "results_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:24.795920Z",
     "start_time": "2020-03-15T17:55:21.728926Z"
    }
   },
   "outputs": [],
   "source": [
    "centers = neighborhoods[['center_lat', 'center_long']].copy().drop_duplicates()\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for _, lat, long in centers.itertuples():\n",
    "    x, y = lonlat_to_xy(long, lat)\n",
    "    xs.append(x)\n",
    "    ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:25.179480Z",
     "start_time": "2020-03-15T17:55:25.171786Z"
    }
   },
   "outputs": [],
   "source": [
    "centers['x'] = xs\n",
    "centers['y'] = ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:25.618199Z",
     "start_time": "2020-03-15T17:55:25.608920Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_fsquare_for_grocery_stores(latitude,\n",
    "                                     longitude,\n",
    "                                     limit=100,\n",
    "                                     radius=30000,\n",
    "                                     query='groceries',\n",
    "                                     credentials=fsq_params):\n",
    "\n",
    "    credentials['ll'] = '{},{}'.format(lat, long),\n",
    "    credentials['limit'] = limit,\n",
    "    credentials['radius'] = radius\n",
    "    credentials['query'] = query\n",
    "    resp = requests.get(url=url, params=credentials)\n",
    "    return {'lat': lat, 'long': long, 'resp': resp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:26.142838Z",
     "start_time": "2020-03-15T17:55:26.138933Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_apply_latlong_to_xy(df):\n",
    "    df = df[['latitude', \"longitude\"]]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for _, lat, long in df.itertuples():\n",
    "        x, y = lonlat_to_xy(long, lat)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    df['x'] = xs\n",
    "    df['y'] = ys\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:29.774464Z",
     "start_time": "2020-03-15T17:55:26.933781Z"
    }
   },
   "outputs": [],
   "source": [
    "groceries = neighborhoods[neighborhoods['gen_cat'] == 'Groceries']\n",
    "first_pass = True\n",
    "\n",
    "print(groceries.shape)\n",
    "for neighborhood in outer_rim:\n",
    "    resp = query_fsquare_for_grocery_stores(**outer_rim.iloc[1, :2].to_dict())\n",
    "    resp = normalize_response(resp)\n",
    "    resp = resp[(resp['category'] == 'Supermarket') |\n",
    "                (resp['category'] == 'Grocery Store')]\n",
    "    if first_pass:\n",
    "        beyond_border_grocery_stores = resp\n",
    "    else:\n",
    "        beyond_border_grocery_stores = pd.concat(\n",
    "            [beyond_border_grocery_stores, resp])\n",
    "        first_pass = False\n",
    "\n",
    "print(beyond_border_grocery_stores.shape)\n",
    "beyond_border_grocery_stores = beyond_border_grocery_stores[[\n",
    "    'latitude', 'longitude'\n",
    "]]\n",
    "beyond_border_grocery_stores = beyond_border_grocery_stores.drop_duplicates()\n",
    "groceries = groceries[['latitude', \"longitude\"]].copy().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:31.398340Z",
     "start_time": "2020-03-15T17:55:31.388694Z"
    }
   },
   "outputs": [],
   "source": [
    "beyond_border_grocery_stores = beyond_border_grocery_stores[[\n",
    "    'latitude', 'longitude'\n",
    "]]\n",
    "beyond_border_grocery_stores = beyond_border_grocery_stores.drop_duplicates()\n",
    "groceries = groceries[['latitude', \"longitude\"]].copy().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:33.436059Z",
     "start_time": "2020-03-15T17:55:33.118995Z"
    }
   },
   "outputs": [],
   "source": [
    "groceries = pd.concat([beyond_border_grocery_stores,\n",
    "                       groceries]).drop_duplicates()\n",
    "groceries = df_apply_latlong_to_xy(groceries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:36.110196Z",
     "start_time": "2020-03-15T17:55:34.378786Z"
    }
   },
   "outputs": [],
   "source": [
    "nearest_grocer = []\n",
    "for _, orig_x, orig_y in centers[['x', 'y']].itertuples():\n",
    "    distances = []\n",
    "    for _, dest_x, dest_y in groceries[['x', 'y']].itertuples():\n",
    "        distance = calc_xy_distance(orig_x, orig_y, dest_x, dest_y)\n",
    "        distances.append(distance)\n",
    "    nearest_grocer.append(min(distances))\n",
    "centers['dist_to_grocer'] = nearest_grocer\n",
    "centers.set_index(['center_lat', \"center_long\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:36.883292Z",
     "start_time": "2020-03-15T17:55:36.873986Z"
    }
   },
   "outputs": [],
   "source": [
    "centers.sort_index(inplace=True)\n",
    "scored_profile.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T17:55:38.108271Z",
     "start_time": "2020-03-15T17:55:38.096033Z"
    }
   },
   "outputs": [],
   "source": [
    "scored_profile['Grocer_Dist'] = centers.distance_to_groceries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T18:39:09.888260Z",
     "start_time": "2020-03-15T18:39:07.443037Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_hists(df, categorical, continuous):\n",
    "    for n in sorted(scored_profile.cluster.value_counts().index):\n",
    "        print(n)\n",
    "        scored_profile[continuous].plot(kind='hist', bins=100, alpha=.3)\n",
    "        scored_profile.loc[scored_profile[categorical] == n, continuous].plot(\n",
    "            kind='hist', bins=100, alpha=.3)\n",
    "        plt.title('{} {}'.format(categorical, n))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_hists(scored_profile, 'cluster', 'Grocer_Dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T18:52:04.051712Z",
     "start_time": "2020-03-15T18:52:03.767224Z"
    }
   },
   "outputs": [],
   "source": [
    "scored_profile['scaled_dist'] = None\n",
    "\n",
    "for n in range(scored_profile.cluster.nunique()):\n",
    "    print(n)\n",
    "    clust = scored_profile.loc[scored_profile['cluster'] ==\n",
    "                               n, [\"Grocer_Dist\", \"scaled_dist\"]]\n",
    "    print(clust.shape)\n",
    "    scaler = StandardScaler()\n",
    "    clust['scaled_dist'] = scaler.fit_transform(\n",
    "        clust['Grocer_Dist'].values.reshape(-1, 1))\n",
    "    scored_profile.loc[scored_profile['cluster'] ==\n",
    "                       n, 'scaled_dist'] = clust['scaled_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T18:52:50.484849Z",
     "start_time": "2020-03-15T18:52:48.131712Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hists(scored_profile, 'cluster', 'scaled_dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T19:10:44.439238Z",
     "start_time": "2020-03-15T19:10:44.409888Z"
    }
   },
   "outputs": [],
   "source": [
    "first_pass = True\n",
    "for c in scored_profile['cluster'].value_counts().index:\n",
    "    clust = scored_profile.loc[scored_profile['cluster'] == c, \"scaled_dist\"]\n",
    "\n",
    "    if first_pass:\n",
    "        prospective_locations = clust.sort_values(ascending=False)[:3]\n",
    "\n",
    "    else:\n",
    "        prospective_locations = pd.concat(\n",
    "            [prospective_locations,\n",
    "             clust.sort_values(ascending=False)[:3]])\n",
    "    first_pass = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T23:40:41.038171Z",
     "start_time": "2020-03-15T23:40:39.005088Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del results_map\n",
    "except:\n",
    "    pass\n",
    "colors = {\n",
    "    0: 'red',\n",
    "    1: 'green',\n",
    "    2: 'navy',\n",
    "    3: 'orange',\n",
    "    4: 'violet',\n",
    "    5: \"yellow\",\n",
    "}\n",
    "\n",
    "results_map = folium.Map(location=center[::-1], zoom_start=9)\n",
    "\n",
    "folium.Circle(center[::-1], radius=50050).add_to(results_map)\n",
    "folium.Circle(center[::-1], radius=80000).add_to(results_map)\n",
    "\n",
    "for latlong, cluster in scored_profile['cluster'].iteritems():\n",
    "    label = f'{latlong}'\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Circle(latlong,\n",
    "                  radius=1200 * 1.2,\n",
    "                  popup=label,\n",
    "                  color=colors[cluster],\n",
    "                  opacity=.1,\n",
    "                  fill=True,\n",
    "                  fill_color=colors[cluster],\n",
    "                  fill_opacity=0.25,\n",
    "                  parse_html=False).add_to(results_map)\n",
    "results_map\n",
    "\n",
    "for _, lat, long, in tqdm(\n",
    "        list(groceries[['latitude', 'longitude']].itertuples())):\n",
    "\n",
    "    folium.Circle([lat, long], radius=625, color='Black',\n",
    "                  fill=False).add_to(results_map)\n",
    "\n",
    "for latlong, _ in tqdm(list(prospective_locations.iteritems())):\n",
    "\n",
    "    folium.Circle(latlong, radius=625, color='Red',\n",
    "                  fill=False).add_to(results_map)\n",
    "results_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T23:47:48.730885Z",
     "start_time": "2020-03-15T23:47:48.720612Z"
    }
   },
   "outputs": [],
   "source": [
    "ethan = zipcodes[[\"zipcodes\",'latitude', 'longitude']].loc[zipcodes['zipcodes']==\"12065\"]\n",
    "ethan.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T23:49:33.358397Z",
     "start_time": "2020-03-15T23:49:31.698321Z"
    }
   },
   "outputs": [],
   "source": [
    "folium.Marker(ethan.iloc[0,1:]).add_to(results_map)\n",
    "results_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T22:58:17.118166Z",
     "start_time": "2020-03-16T22:58:17.068828Z"
    }
   },
   "outputs": [],
   "source": [
    "heat_data\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 623.844,
   "position": {
    "height": "40px",
    "left": "1314px",
    "right": "20px",
    "top": "69px",
    "width": "372px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
